{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training_10617.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITNbe4E7yuaa"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from pickle import load\n",
        "import itertools\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXimLkuLz7vk"
      },
      "source": [
        "def encode_sequences(tokenizer, length, lines):\n",
        "\ttokenized = tokenizer.texts_to_sequences(lines)\n",
        "\treturn pad_sequences(tokenized, maxlen=length, padding='post')\n",
        "\n",
        "def tokenize_it(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puic_C0jDYSW"
      },
      "source": [
        "filename = 'spa.txt'\n",
        "\n",
        "file = open(filename, mode='rt', encoding='utf-8')\n",
        "doc = file.read()\n",
        "file.close()\n",
        " \n",
        "lines = doc.strip().split('\\n')\n",
        "pairs = [line.split('\\t')[:2] for line in  lines]\n",
        "\n",
        "cleaned = list()\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "for pair in pairs:\n",
        "  clean_pair = list()\n",
        "  \n",
        "  for line in pair:\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "    line = line.decode('UTF-8')\n",
        "\n",
        "    line = line.split()\n",
        "    line = [word.lower() for word in line]\n",
        "    line = [word.translate(table) for word in line]\n",
        "    line = [re_print.sub('', w) for w in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "\n",
        "    clean_pair.append(' '.join(line))\n",
        "  cleaned.append(clean_pair)\n",
        "\t\n",
        "raw_points = np.array(cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j898T8iHfEvd"
      },
      "source": [
        "samples = 100000\n",
        "\n",
        "dataset = raw_points[:samples, :] #np.vstack((raw_points[:samples//2, :],raw_points[125444:125444+(samples//2),:]))\n",
        "\n",
        "\n",
        "for i in range(0,samples):\n",
        "  words_eng = dataset[i,0].split()\n",
        "  words_spa = dataset[i,1].split()\n",
        "  dataset[i,0] = \" \".join(w for w in words_eng)\n",
        "  dataset[i,1] = \" \".join(w for w in words_spa)\n",
        "\n",
        "np.random.shuffle(dataset)\n",
        "train, test = dataset[:int(samples*0.95)], dataset[int(samples*0.95):]\n",
        "\n",
        "tokenized_eng = tokenize_it(dataset[:, 0])\n",
        "size_eng = len(tokenized_eng.word_index) + 1\n",
        "len_eng = max_length(dataset[:, 0])\n",
        "\n",
        "tokenized_spa = tokenize_it(dataset[:, 1])\n",
        "size_spa = len(tokenized_spa.word_index) + 1\n",
        "len_spa = max_length(dataset[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNosQtxYX8y9"
      },
      "source": [
        "a = np.zeros((50,1))\n",
        "max_idx = 0\n",
        "for line in train[:,0]:\n",
        "  idx = len(line.split())\n",
        "  a[idx] = a[idx] + 1\n",
        "\n",
        "  if idx > max_idx:\n",
        "    max_idx = idx\n",
        "\n",
        "merged = list(itertools.chain(*a.tolist()))\n",
        "plt.bar([i for i, _ in enumerate(merged)], merged)\n",
        "plt.xlabel('Length of sentences')\n",
        "plt.ylabel('Number of datapoints')\n",
        "plt.title('Distribution of length of sentences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyg67MQjwXiC"
      },
      "source": [
        "trainX = encode_sequences(tokenized_eng, len_eng, train[:, 0])\n",
        "trainY = encode_sequences(tokenized_spa, len_spa, train[:, 1])\n",
        "\n",
        "testX = encode_sequences(tokenized_eng, len_eng, test[:, 0])\n",
        "testY = encode_sequences(tokenized_spa, len_spa, test[:, 1])\n",
        "\n",
        "train_outY = trainY\n",
        "test_outY = testY\n",
        "\n",
        "for index in range(0,len(train_outY)):\n",
        "  train_outY[index] = np.asarray([0]+list(train_outY[index,:-1]))\n",
        "\n",
        "for index in range(0,len(test_outY)):\n",
        "  test_outY[index] = np.asarray([0]+list(test_outY[index,:-1]))\n",
        "  \n",
        "trainY = encode_sequences(tokenized_spa, len_spa, train[:, 1])\n",
        "testY = encode_sequences(tokenized_spa, len_spa, test[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if1N0May9XpF"
      },
      "source": [
        "import keras.utils\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNCbMNzBHTIy"
      },
      "source": [
        "def norm_model(eng_size, spa_size, eng_len, spa_len, n_units,dropout_rate,eng_embed_dim,es_embed_dim):\n",
        "  \n",
        "  input_one = Input(shape=(eng_len,))\n",
        "  input_two = Input(shape=(spa_len,)) \n",
        "\n",
        "  emb_one = Embedding(input_dim=eng_size,output_dim=eng_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_en\")(input_one)\n",
        "  emb_two = Embedding(input_dim=spa_size,output_dim=es_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_es\")(input_two)\n",
        "\n",
        "  _, state_h, state_c = LSTM(n_units, recurrent_dropout = dropout_rate, return_state = True)(emb_one)\n",
        "\n",
        "  lstm_decode = LSTM(n_units, dropout=dropout_rate, return_sequences=True)(emb_two, initial_state = [state_h,state_c])\n",
        "  out = Dense(spa_size, activation='softmax')(lstm_decode)\n",
        "  out_model = Model([input_one,input_two],out)\n",
        "\n",
        "  return out_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ryqldmPCKOt"
      },
      "source": [
        "def att_model(eng_size, spa_size, eng_len, spa_len, n_units,dropout_rate,eng_embed_dim,es_embed_dim):\n",
        "  \n",
        "  input_one = Input(shape=(eng_len,))\n",
        "  input_two = Input(shape=(spa_len,)) \n",
        "\n",
        "  emb_one = Embedding(input_dim=eng_size+1,output_dim=eng_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_en\")(input_one)\n",
        "  emb_two = Embedding(input_dim=spa_size+1,output_dim=es_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_es\")(input_two)\n",
        "\n",
        "  emb_one = Bidirectional(LSTM(int(n_units/2),recurrent_dropout=dropout_rate,return_sequences=True))(emb_one)\n",
        "  x_enc,state_h,state_c = LSTM(n_units,return_state = True,recurrent_dropout = dropout_rate, return_sequences=True)(emb_one)\n",
        "\n",
        "  lstm_decode = LSTM(n_units,dropout=dropout_rate, return_sequences=True)(emb_two,initial_state = [state_h,state_c])\n",
        "  lstm_decode = LSTM(n_units,dropout=dropout_rate, return_sequences=True)(lstm_decode)\n",
        "\n",
        "  attention = Activation('softmax', name='attention')(dot([lstm_decode, x_enc], axes=[2, 2]))\n",
        "\n",
        "  output = TimeDistributed(Dense(2*n_units, activation=\"tanh\"))(concatenate([dot([attention, x_enc], axes=[2,1]), lstm_decode]))\n",
        "  out = TimeDistributed(Dense(spa_size, activation=\"softmax\"))(output)\n",
        "\n",
        "  out_model = Model([input_one,input_two],out)\n",
        "\n",
        "  return out_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFS0I0QUxSk"
      },
      "source": [
        "model = norm_model(size_eng, size_spa, len_eng, len_spa, 256, 0.2, 256, 256)\r\n",
        "keras.utils.plot_model(model, to_file='Normal_model.png', rankdir='LR', show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gla4qrA5Vpzl"
      },
      "source": [
        "model = att_model(size_eng, size_spa, len_eng, len_spa, 256, 0.2, 256, 256)\r\n",
        "keras.utils.plot_model(model, to_file='att_model.png', rankdir='LR', show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4fDZ19_6DZZ"
      },
      "source": [
        "n_units = [128, 256]\n",
        "history_list = []\n",
        "\n",
        "eng_embed_dim = 256\n",
        "es_embed_dim = eng_embed_dim\n",
        "dropout_rate = 0.2\n",
        "\n",
        "for el in n_units:\n",
        "  # model = att_model(size_eng, size_spa, len_eng, len_spa, el, 0.2, 256, 256)\n",
        "  model = norm_model(size_eng, size_spa, len_eng, len_spa, el, 0.2, 256, 256)\n",
        "\n",
        "  checkpoint = ModelCheckpoint('model_norm_{}.h5'.format(el), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model.compile(loss=SparseCategoricalCrossentropy(),optimizer=optimizers.Adam(lr=0.001))\n",
        "\n",
        "  history = model.fit([trainX,train_outY], trainY, epochs=30, batch_size=256, validation_data=([testX,test_outY], testY), callbacks=[checkpoint])\n",
        "  history_list.append(history)\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "for i in range(len(history_list)):\n",
        "  train_loss.append(history_list[i].history['loss'])\n",
        "  train_acc.append(history_list[i].history['accuracy'])\n",
        "  val_loss.append(history_list[i].history['val_loss'])\n",
        "  val_acc.append(history_list[i].history['val_accuracy'])\n",
        "\n",
        "np.savez('history_mod.npz', train_loss=train_loss, train_acc=train_acc, val_loss=val_loss, val_acc=val_acc, n_units=n_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd7yZc5bFDE7"
      },
      "source": [
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5NvbR5FV0V"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}